Финальный проект по машинному обучению за курс

Клем Кирилл (https://githubcom/KirillKlem), Линьков Вадим (https://githubcom/aisatsanz)

# Wine Quality ML: регрессия качества и классификация типа вина

## 1 Постановка задачи

Датасет: UCI Wine Quality (Vinho Verde, красные и белые вина)  
Цели:

1 **Регрессия**: предсказать дегустационную оценку `quality` (0–10) по химическим показателям
2 **Классификация**: по тому же признаковому пространству предсказать **тип вина** (`red` vs `white`)
3 Построить интерпретируемое решение: аномалии, feature engineering, SHAP / Shapley Flow, влияние признаков
4 Оценивать регрессию не только по RMSE/MAE/R^2, но и по **Quadratic Weighted Kappa (QWK)** - как в Kaggle Playground Series S3E5



## 2 Данные и базовая обработка

- Источник: два CSV-файла `winequality-redcsv` и `winequality-whitecsv` (UCI)
- Объединены в один датафрейм, добавлен категориальный признак `type ∈ {red, white}`
- Пропусков нет, дубликаты строк удалены (~ 1.1% записей)
- Все исходные признаки числовые: кислотность, сахар, хлориды, диоксид серы, плотность, pH, сульфаты, алкоголь

Основные наблюдения по распределениям:

- Сильные правые хвосты у `residual sugar`, `chlorides`, `free/total sulfur dioxide`, `sulphates`
- `density` и `pH` лежат в очень узких диапазонах
- Распределения сильно отличаются между `red` и `white` (особенно по кислотам, сахару и алкоголю)



## 3 Исследовательский анализ (EDA)

### 3.1 Связь признаков с `quality`

Корреляции:

- Наиболее сильная положительная связь с качеством:
  - `alcohol` (~+0.47)
  - `sulphates` (~+0.28)
- Сильные отрицательные:
  - `density` (~–0.33)
  - `volatile acidity` (~–0.27)

Ключевые внутренние корреляции:

- `free sulfur dioxide` vs `total sulfur dioxide` (~+0.72) - почти дублирующие показатели
- `density` vs `residual sugar` (~+0.52) - плотность растёт с сахаром
- `density` vs `alcohol` (~–0.67) - спирт легче воды
- `total sulfur dioxide` vs `residual sugar` (~+0.49) - сладкие вина часто требуют больше SO2

Boxplot’ы по `quality` показывают:

- Рост `quality` сопровождается:
  - снижением `volatile acidity`
  - снижением `density`
  - ростом `alcohol`
- Остальные признаки либо почти не меняются с качеством, либо шумные

### 3.2 Различия между красными и белыми

Из KDE и PCA:

- Красные вина:
  - выше `fixed acidity` и `volatile acidity`
  - более "сухие" (`residual sugar` заметно ниже)
  - чуть выше и стабильнее по `alcohol`
- Белые:
  - больше остаточного сахара, длинный сладкий хвост
  - ниже `chlorides`
  - слегка выше `density` и более вариативный `alcohol`

PCA (2 компоненты) хорошо разделяет типы вдоль PC1 --> тип вина легко восстанавливается по химпрофилю



## 4 Обнаружение и кодирование аномалий

### 4.1 Статистические выбросы

Использовано несколько критериев:

- **Z-score** по каждому признаку, порог |Z| > 3
- **IQR-выбросы** (outside [Q1 − 1.5 IQR; Q3 + 1.5 IQR]) - даёт много редких, но физически правдоподобных вин
- **Тест Граббса** по каждому признаку - единичные экстремумы, статистически значимые
- Явно кодируем аномальность:

  - бинарные флаги `is_iqr_outlier_*`
  - флаги `is_grubbs_extreme_*`
  - счётчик `iqr_outlier_count`

### 4.2 Геометрические аномалии

- Строим стандартизованное пространство базовых признаков
- Считаем kNN-дистанции (k=5) --> **`knn_anomaly_score`** как среднее расстояние до соседей

Эти признаки используем как дополнительные "аномальные" фичи в моделях



## 5 Генерация признаков

### 5.1 Target encoding категориального признака

- Единственный категориальный признак - `type` (цвет)
- K-fold target encoding (5 фолдов) --> признак **`color_te`**
- Сравнение с one-hot (`type_red`, `type_white`) через LGBM:
- target encoding даёт меньшую MSE, поэтому дальше используем именно его

### 5.2 KNN-таргет-признаки

Для каждого экземпляра:

- `knn_target_mean` - среднее качество ближайших соседей по химпрофилю
- `knn_target_std` - разброс качества соседей
- `knn_avg_distance` - средняя дистанция до соседей

Сделано честно через K-fold loop, чтобы избежать лика

Добавление этих признаков снижает MSE в кросс-валидации: у модели появляется "локальное представление" о типичном качестве похожих вин

### 5.3 Доменно-информированные комбинации

Сгенерированы фичи, основанные на энологии:

- `SO2_ratio = free sulfur dioxide / total sulfur dioxide`  
  (сколько свободного SO2 относительно общего консерванта)
- `sugar_density = residual sugar / density`  
  (условная "сладость при данной плотности")
- `acid_balance = fixed acidity + citric acid - volatile acidity`  
  (баланс "структурной" и летучей кислотности)
- `alcohol_efficiency = alcohol / (residual sugar + 1e-5)`  
  (какая доля сахара ушла в спирт)
- `mineral_index = chlorides * density`  
  (суррогат "минеральности")
- `ph_acid_interaction = pH * fixed acidity`  
  (связь между крепостью кислоты и её концентрацией)
- Отклонения от средних по типу:
  - `alcohol_color_dev = alcohol − mean_alcohol(type)`
  - `acid_color_dev   = fixed acidity − mean_fixed_acidity(type)`

Корреляции и F-test/chi^2 показывают, что большинство этих признаков имеет значимую связь с качеством и попадает в список сильных фичей



## 6 Отбор признаков и финальный фичеспейс

Использованы несколько независимых процедур:

1. **Корреляции и F-test (ANOVA)** - ранжирование по линейной связи с `quality`
2. **χ²-статистика** (на нормализованных X) - для проверки нелинейных зависимостей
3. **RFECV (LGBMRegressor)** - рекурсивный отбор с кросс-валидацией
4. **Optuna** - поиск подмножества фич, оптимизирующего MSE LGBM по CV
5. **Lasso** (на стандартизованных признаках) - отбор по ненулевым коэффициентам
6. **Стабильность feature importance**:
   - 10 бутстрап-подвыборок
   - для каждой обучается LGBM
   - считаем среднюю важность и `stability_ratio = mean/std`

Финальный набор признаков `final_features` - **объединение**:

- всех фич, выбранных RFECV
- всех фич с ненулевыми коэффициентами Lasso
- топ-фич по среднему feature importance и по Optuna

Из него вручную исключены:

- `target_bin` (происходил от бинаризации качества, не нужен в регрессии)
- `is_red` (тип вина кодируем через более содержательные признаки и target encoding)



## 7 SHAP, Shapley Flow и аномалии по объяснениям

### 7.1 SHAP для линейной модели и LGBM

- Для Ridge (на стандартизованных признаках) строились SHAP-значения через `LinearExplainer`
- Для LGBM - `TreeExplainer`

Выводы:

- Глобально **главный драйвер качества - `knn_target_mean`**  
  (то есть "среднее качество похожих вин")
- Далее по вкладу идут:
  - кислотность и алкоголь: `volatile acidity`, `alcohol`, `alcohol_color_dev`, `acid_balance`
  - технологические параметры: `sulphates`, `SO2_ratio`, `free/total sulfur dioxide`, `color_te`
- Ridge в большей степени опирается на "сырые" признаки (сахар, плотность, кислотность). LGBM активно использует нелинейные взаимодействия (отношение SO2, отклонение алкоголя от среднего и тд)


### 7.2 SHAP-аномалии

- Для каждого объекта считаем суммарную интенсивность влияния  
  `shap_intensity = sum(|SHAP_i|)` по финальным признакам
- Верхние 3% по `shap_intensity` помечаются как `is_shap_outlier = 1`

Результат:

- Эти объекты концентрируются на краях в SHAP-PCA пространстве
- Удаление SHAP-аномалий и повторное обучение LGBM **уменьшает RMSE** в кросс-валидации: модель не пытается подстроиться под редкие, сложно объяснимые случаи

### 7.3 SHAP-эмбеддинги и Shapley Flow

1. **SHAP-эмбеддинг**:
   - PCA на матрице SHAP (по признакам) --> 10 компонент `shap_pc1..10`
   - Кросс-валидация LGBM:
     - только исходные признаки --> ошибка ~0.28 
     - только SHAP-эмбеддинги --> ~0.11
     - конкатенация исходных + SHAP --> ~0.10

   Интерпретация: в SHAP-пространстве зависимость с таргетом гораздо ближе к линейной, чем в исходном химическом пространстве.

2. **Shapley Flow**:
   - По матрице |corr(SHAP)| строится граф связи признаков
   - Агломеративная кластеризация признаков (4 кластера)
   - Для каждого объекта суммируем SHAP-значения по кластерам --> 4-мерный **flow-эмбеддинг** `shap_flow_0..3`
   - LGBM:
     - только Shapley-Flow эмбеддинги --> ошибка ~0.097
     - исходные + Shapley-Flow --> ~0.082
     - исходные + номер flow-кластера как отдельный категориальный признак --> ошибка снижается, но меньше, чем при использовании самих flow-координат


## 8 Финальные фичи и объединённый датасет

Финальный датафрейм `df_final` собирает:

- `final_features` (инженерные и отобранные признаки)
- аномальные флаги и показатели:
  - `is_iqr_outlier_*`, `is_grubbs_extreme_*`,
  - `iqr_outlier_count`, `knn_anomaly_score`
- Shapley Flow эмбеддинг `shap_flow_*`
- номер flow-кластера `cluster_flow`
- флаг SHAP-аномалии `is_shap_outlier`

Для обучения финальных моделей:

- берём только объекты без SHAP-аномалий (`is_shap_outlier == 0`)
- регрессионные признаки: `reg_features = final_features ∪ anomaly_feats ∪ flow_feats ∪ {cluster_flow}`

## 9 Модели и валидация

### 9.1 Регрессия (оценка качества вина)

Модели:
- `Ridge`, `ElasticNet` (на стандартизованных признаках)
- `KNNRegressor`
- `SVR (RBF kernel)`
- `RandomForestRegressor`
- `LightGBMRegressor` (градиентный бустинг по деревьям)

Валидация:

- K-fold (k=5), стратификация по `quality` не использовалась (регрессия)
- Метрики:
  - MSE / RMSE (как в коде)
  - MAE
  - R^2
  - Quadratic Weighted Kappa (QWK)

Функция QWK реализована по формуле из описания Kaggle Playground Series S3E5: строится матрица наблюдаемой совместной частоты (O), матрица ожидаемой (E), весовая матрица W с квадратичной пенализацией по разности оценок, далее:

\[
\kappa = 1 - \frac{\sum_{i,j} w_{ij} O_{ij}}{\sum_{i,j} w_{ij} E_{ij}}
\]

Числа:

- По эксперименту только с LGBM и разными представлениями признаков:
  - только исходные признаки: ошибка ~0.28
  - только SHAP-эмбеддинги: ~0.11
  - исходные + SHAP-эмбеддинги: ~0.10
  - только Shapley-Flow эмбеддинги: ~0.097
  - исходные + Shapley-Flow эмбеддинги: ~0.082

Финальная модель (по комбинации RMSE и QWK):

- LightGBM на полном фичеспейсе (включая Shapley Flow) и без SHAP-аномалий
- Линейные модели, KNN и SVR заметно уступают по RMSE и QWK RF ближе к LGBM, но проигрывает из-за меньшей гибкости и менее устойчивых фич-важностей

Важно: полученные метрики **не напрямую сопоставимы** с Kaggle-соревнованием:

- используется полный UCI-датасет, а не срез из соревнования
- кросс-валидация локальная, а не официальный hidden test
- мощные фичи типа `knn_target_mean` фактически приближают условное математическое ожидание `E[quality | X]` и сильно поднимают метрики

### 9.2 Классификация типа вина (red vs white)

Цель: проверить, насколько хорошо **финальные признаки** отделяют тип вина

- Цель: `y_clf_type = 1 если type == 'red'`, 0 иначе
- Признаки: тот же `X_reg` (финальный фичеспейс, без SHAP-аномалий)
- Модели:
  - `LogReg`, `KNN`, `SVM (RBF)`, `RandomForest`, `LightGBM`
- Валидация: 5-fold, метрики ACC, F1, ROC-AUC

Результат:

- Все модели показывают очень высокие ROC-AUC. Красные и белые вина практически линейно разделимы по химическим признакам
- Добавление аномальных фичей и Shapley Flow почти не меняет качество классификации - тип вина легко определим уже по базовым/инженерным признакам



## 10 Ключевые выводы

1. Химический профиль красных и белых вин принципиально различается, что позволяет почти безошибочно классифицировать тип вина
2. Для регрессии качества:
   - Простые модели на сырых признаках дают умеренное качество
   - Добавление доменно-информированных фичей и kNN-таргет-признаков существенно снижает ошибку
   - Переход к SHAP-эмбеддингам и Shapley Flow даёт ещё один заметный шаг вперёд
3. Аномалии важно не только находить, но и явно кодировать:
   - флаги выбросов и `knn_anomaly_score` помогают моделям не "перегибать" общую зависимость
   - фильтрация SHAP-аномалий стабилизирует обучение и улучшает метрики
4. SHAP / LIME дают согласованную картину важности признаков:
   - главное - локальный контекст (`knn_target_mean`) и связка "алкоголь + кислотность + SO2"
   - доменные комбинации (`acid_balance`, `SO2_ratio`, `alcohol_efficiency`) действительно несёт много информации
5. По сравнению с базовыми моделями из исходного датасета, итоговый пайплайн делает большое число шагов:
   - структурное feature engineering
   - многоступенчатый отбор фич
   - аномалия-aware обучение
   - интерпретация через SHAP / Shapley Flow


## 11 Как воспроизвести

1. Склонировать репозиторий и скачать архив с данными UCI (winequality-red/white) (внутри есть unzip если что)
2. Открыть итоговый notebook
3. Установка доп зависимостей внутри notebook
4. notebook ячейки должны идти последовательно и выполняться без ошибок
